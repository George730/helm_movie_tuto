
# Milestone2 Updates to Data/Pipeline

## Pipeline Overview:
Every 12 hours run the following scripts in this order:
1. get_last_12_hours.sh
    - This script goes to kafka and pulls the last 12 hours worth of data. It returns one large csv with all reccomendation, rating, and data requests for this time period. It does not take any parameters.
2. split_data_by_type.py
    - This script goes through the massive csv generated by the last script and splits the data into 3 csv's. Idea here is that this makes it easy to open a certain type of data in a pandas dataframe using pd.read_csv(somefilename). It also does the data cleaning, only keeping the useful information from any kafka data (i.e. user id, rating score, minute watched, movie title/year). It also reduces the amount of data by only taking the max minute watched for a specific user watching a movie (this is useful because we get one request per minute when a user watches a movie and by doing this we can get rid of a ton of redundant data). Actually specs on a random 12 hours on how much this reduces data: original csv is 1990327kb, the sum of the 3 new csvs 140733, resulting in a 90% size reduction. Finally, it keeps track of a set of movies watched by any user during this time period. This will be useful for the next script. This script returns 3 csvs and one text file and takes not input parameters.
3. update_movies.py
    - This script updates our local movie csv file. It uses the set of movies from the previous script and compares it with the set of movies that we already have in our movie csv file. For any movies that have been seen but do not appear in the csv it makes the api calls to kafka and tmdb to get more information and then appends it to the csv. 
4. TODO: write a script that deletes the oldest file
    

Every week or 2 weeks run update_users.py to update the data we have gathered from kafka about the users. There are ~1,000,000 users right now so this script kind of takes forever (like 40 minutes), but its way easier than querying users individually everytime we pull data.

## Info on Columns in Each DataFrame
- date_hour_data.csv columns:
    - DTG = date time group that the movie data was requested
    - user_id = the user id of the user requesting the data
    - title_year = the title/year released of the movie the user requested data for
    - max_minute_watched = the max minute a specific user watched for a specific movie
- date_hour_rating.csv columns:
    - DTG = date time group that the movie data was requested
    - user_id = the user id of the user requesting the data 
    - title_year = the title/year released of the movie the user requested data for 
    - user_rating = the user rating of the movie on a scale of 1-5
- date_hour_recommendation.csv columns:
    - DTG = date time group that the movie data was requested
    - user_id = the user id of the user requesting the data data for 
    - rec1: our first movie recommendation (in title+year format)
    - rec2: our second movie recommendation (in title+year format)
    - rec3: our third movie recommendation (in title+year format)
    - ...
    - rec19: our 19th movie recommendation (in title+year format)
    - rec20: our 20th movie recomendation (in title+year format)
- user_data.csv columns:
    - user_id = the user id of the user requesting the data 
    - age = the users age
    - occupation = the users occupation
    - gender = the users gender
- movie_data.csv columns:
    - title_year = the title/year released of the movie the user requested data for 
    - tmdb_id = the true movie database id
    - title = the striaght title
    - overview = a summary of the movie
    - genres = the genres of the movie as a list
    - cast = the three main actors as a list
    - crew = the director (in list form)
    - keywords = the keywords of the movie as a list
    - popularity
    - release_date 
    - runtime
    - vote_average
    - vote_count

## Merging Datasets
It is easy to merge any two (or even three data sets using a left/right join)

Ex1: if you wanted to augment the rating data with the user information, you would use the following command:
ratings_with_user = rating.merge(user, on='user_id', how='left')

Ex2: if you wanted to augment the rating data with the movie information, you would use the following command:
rating_with_movie = rating.merge(movie, on='title_year', how='left')

Ex3: if you wanted to augment the rating data with both the movie and user information you would use the following commands:
rating_with_movie_and_user = rating.merge(movie, on='title_year', how='left').merge(user, on='user_id', how='left')



## HELPFUL LINKS USED:
https://stackoverflow.com/questions/62599036/python-requests-is-slow-and-takes-very-long-to-complete-http-or-https-request
https://stackoverflow.com/questions/60882882/how-to-consume-messages-between-two-timestamps-using-kafka-console-consumer#:~:text=Or%2C%20if%20your%20brokers%20can,to%20kafka%2Dconsole%2Dconsumer.
